--- dqn_project.py
+++ dqn_project.py
@@ -52,9 +52,7 @@
         # define yours training operations here...
         self.observation_input = tf.placeholder(tf.float32, shape=[None] + list(self.env.observation_space.shape))
         self.keep_prob = tf.placeholder(tf.float32)
-        self.action_input = tf.placeholder(tf.float32, [None, self.env.action_space.n])
-        self.q_values = self.build_model(self.observation_input)
-        self.y = tf.placeholder(tf.float32, [None])
+        q_values = self.build_model(self.observation_input)
 
         # define your update operations here...
         self.ini_random_walk_prob = 1.0
@@ -114,9 +112,7 @@
         TODO: Implement the functionality to update the network according to the
         Q-learning rule
         """
-        q_learning_action = tf.reduce_sum(tf.multiply(self.q_val, self.action_input), 1)
-        self.loss = tf.reduce_mean(tf.square(tf.subtract(self.y, q_learning_action)))
-        self.optimizer = tf.train.AdamOptimizer(1e-4).minimize(self.loss)
+        raise NotImplementedError
 
     def train(self):
         """
@@ -127,22 +123,13 @@
             2. Updating the network at some frequency
             3. Backing up the current parameters to a reference, target network
         """
-        # Initially have some random walks and make a replay memory
-        for episode in range(1000):
-            done = False
-            obs = env.reset()
-            while not done:
-                action = random.randint(0, self.env.action_space.n - 1)
-                encoded_action = np.zeros(self.env.action_space.n)
-                encoded_action[action] = 1
-                # action = self.select_action(obs, evaluation_mode=False)
-                next_obs, reward, done, info = env.step(action)
-                self.replay_memory.append((obs, encoded_action, reward, next_obs, done))
-                obs = next_obs
-                if len(self.replay_memory) > RE
-
-                self.num_steps += 1
-            self.num_episodes += 1
+        done = False
+        obs = env.reset()
+        while not done:
+            action = self.select_action(obs, evaluation_mode=False)
+            next_obs, reward, done, info = env.step(action)
+            self.num_steps += 1
+        self.num_episodes += 1
 
     def eval(self, save_snapshot=True):
         """
