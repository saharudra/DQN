Network Used:
For the first task: A 3 layer fully connected network with 512 and 256 neurons in the first and the second hidden layer.
A tappered network is used. Played around with the number of neruons in the two hidden layers and also with adding another
layer. The 4-layer architecture takes more time to converge.
Using relu non-linearity and dropout in the network.
Played around with the value of drop out.
At 0.15 prob of dropping the neurons, the average reward seems to take more time to converge. This can be attributed to
the noise that dropout brings in the network.

Avg reward: -4.0862866861
Avg reward: -231.108268566
Avg reward: -186.755520869
Avg reward: -174.954395989
Avg reward: -154.506499655
Avg reward: -150.223011341
Avg reward: -135.634729678
Avg reward: -124.754545109
Avg reward: -118.704945252
Avg reward: -117.436965616
Avg reward: -132.128643504
Avg reward: -139.133213961
Avg reward: -149.554769123
Avg reward: -108.844266972
